; main section, mandatory
[experiment]

; ltd_type is the algorithm generation process that the agent will use
; LtD means 'Learning to Delegate', an previous naming used on our approach
; Allowed types are: uniform or gaussian
ltd_type = gaussian

; team_sizes is the number of algorithms in the agent's portfolio
; use comma-separated values to test various team sizes
team_sizes = 25

; bandit_sizes is the number of actions of the underlying problem (a multi-armed bandit)
; use comma-separated values to test various number of actions
bandit_sizes = 100

; upper_bounds: algorithms generated with uniform have a probability of selecting the best action sampled from U(0, upper_bound)
; This parameter is ignored for gaussian
; use comma-separated values to test various upper_bounds
upper_bounds = 0.30,0.40,0.50,0.60,0.70,0.80,0.90

; trials is the number of interactions with the multi-armed bandit problem.
; The more trials, the higher the chance learning over actions overcomes learning over algorithms
; use comma-separated values to test various number of trials
trials = 10000

; executions is the number of repetitions of each parameter configuration.
; For example, if the parameters are: upper_bounds = 0.1,0.3,0.5; team_sizes = 10,25;
; bandit_sizes = 100,250; trials = 1000,5000 then there will be 3 x 2 x 2 x 2 = 24 experiments
; if executions = 100, each experiment will be repeated 100 times and averaged, giving 2400 executions in total
executions = 1000

; max_parallel_setup is the maximum number of experiment creation procedures to run in parallel
; currently, it is recommended to leave as 1 because there has been some problems with more
max_parallel_setup = 1

; max_parallel_runs is the maximum number of experiment execution prodecures to run in parallel.
; It is recommended that this parameter does not exceed the # of cores in your machine
max_parallel_runs = 4

; alpha is the initial learning rate for both agents learning over actions or algorithms
alpha = 1

; aplha_decay is the multiplicative factor by which the initial learning rate is multiplied after each trial on the multi-armed bandit.
; To calculate this value, you can stipulate a desired value for alpha and then use the formula:
; alpha_decay = (final_alpha / initial_alpha)^ (1/trials)
; to keep a constant learning rate over all trials, leave this value as 1
; This example value is for decaying alpha from 1 to 0.01 in 5000 trials
alpha_decay = 0.999

; epsilon is the initial exploration probability for the learning agents
epsilon = 0.1

; epsilon_decay is the multiplicative factor by which the initial epsilon is multiplied after each trial on the multi-armed bandit.
; To estimate a proper decay, or to keep epsilon constant, the same rules of alpha_decay apply.
epsilon_decay = 0.999

; The directory where the experiment results will be written. It is created in a relative path
; based on where the python interpreter is being called. Inside this directory, many subdirectories
; will be created, each with the results of a parameter configuration.
; For uniform, the directories will look like: bandit_size/team_size/upper_bound for each variation in these values.
; For gaussian, the directories will look like: bandit_size/team_size/mu for each variation in these values.
; Additionally, for main_sequential, the directories will be preceded by the identifier used in command line.
output_dir = uniform_u
